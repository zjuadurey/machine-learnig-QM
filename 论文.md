# 毕设论文

## 简介

量子多体问题是一类广泛物理问题的总称，这类问题研究由许多相互作用粒子组成的微观系统性质，其微观性质需要使用量子力学来提供对系统的准确描述。

量子相变（QPT）是绝对零度温度下量子多体系统的显着表现，在该系统中量子涨落起主导作用，不存在热涨落[1]。QPT可以通过改变哈密顿量的参数来实现，例如外部磁场或粒子间耦合常数。当控制参数通过临界值变化时，系统的基态会随之发生突变。如何揭示和表征量子多体系统的临界现象是一项重要任务，已成为凝聚态物理学的热点。

一般来说，虽然控制每个粒子运动的基本物理定律相对会简单一些，但对粒子集合的研究可能非常复杂。在多体量子系统中，粒子之间的重复相互作用会产生量子相关性或纠缠。因此，系统的波函数是一个包含大量信息的复杂对象，这使得精确分析或计算十分困难，甚至无法实现。因此，量子多体问题的求解通常依赖于一组针对特定问题的近似值。寻找合适的方法较为准确地求得体系中某些物理量的近似值便是我们的目标。

## 绪论

### 量子多体问题

量子多体问题是一类广泛物理问题的总称，这类问题研究由许多相互作用粒子组成的微观系统性质，其微观性质需要使用量子力学来提供对系统的准确描述。

一般来说，虽然控制每个粒子运动的基本物理定律相对会简单一些，但对粒子集合的研究可能非常复杂。在多体量子系统中，粒子之间的重复相互作用会产生量子相关性或纠缠。因此，系统的波函数是一个包含大量信息的复杂对象，这使得精确分析或计算十分困难，甚至无法实现。因此，量子多体问题的求解通常依赖于一组针对特定问题的近似值。寻找合适的方法较为准确地求得体系中某些物理量的近似值便是我们的目标。

### 量子相变

量子相是物质在零温度下的量子态。即使在零温度下，量子力学系统也具有量子涨落。

量子相变（QPT）是不同量子相（也就是零温度下的物质相）之间的相变。有别于传统相变，量子相变只能通过在绝对零度温度下改变物理参数（如磁场或压力）来实现。量子相变
描述了由于量子涨落而导致的多体系统基态的突然变化。

这些状态与经典物质状态之间的区别在于，在经典物理中，材料表现出的不同物相最终取决于温度和密度的变化或材料的其他一些宏观性质，而量子相可以随着不同类型的序参数的变化而变化（序参量是哈密顿量中的一个参数）。

谈论量子相变意味着谈论T = 0时的跃迁：通过调整压力，化学成分或磁场等非温度参数，可以将诸如居里温度之类的一些转变温度抑制到0 K。由于在零温度下处于平衡状态的系统始终处于其最低能量状态，因此量子相变不能用热运动解释。

虽然绝对零度在物理上无法实现，但可以在系统靠近临界点的低温行为中检测到跃迁的特征。在非零温度下，能量尺度为kBT的经典波动与能标度ħω的量子涨落竞争。这里ω是量子振荡的特征频率，与相关时间成反比。量子涨落主导着系统在ħω>kBT的区域（称为量子临界区域）的行为。

量子相变可以通过改变哈密顿量的参数来实现，例如外部磁场或粒子间耦合常数。当控制参数通过临界值变化时，系统的基态会随之发生突变。因此，研究量子相变的一个基础是计算出量子多体系统的基态能量。量子多体问题的求解通常依赖于一组针对特定问题的近似值。

### 变分法

在量子力学中，求解一个系统的基态尤其是求解基态能量，有最为重要的作用。因为一个系统的基态或低激发态常常决定了系统的宏观行为。但是能够求解的量子力学系统少之又少。因此人们发展了一系列近似计算方法。变分法是找到最低能量特征态或基态以及一些激发态近似值的一种重要近似方法方法。这种方法的基础是变分原理。<https://en.wikipedia.org/wiki/Variational_method_(quantum_mechanics)#cite_note-1>的1

变分法根据一个或多个参数选择“试验波函数”，并找到使得能量期望值尽可能低的一组参数的值。通过将参数固定到该组数值而获得的波函数便视为基态波函数的近似值，此外，该状态下能量的期望值是真实基态能量的上限。

给定一个希尔伯特空间和一个系统的哈密顿量$H$，忽略连续频谱的复杂性，我们关注哈密顿量H的离散频谱与每个特征值的相应特征空间：
$$\langle\psi_{\lambda_1}|\psi_{\lambda_2}\rangle=\delta_{ij}$$
其中：
$$\delta_{ij}=\begin{cases}
0, & i \neq j\\
1, & i = j\\
\end{cases}$$

哈密顿量通过以下关系与$\lambda$相关:
$$\hat{H}|\psi_{\lambda}\rangle=\lambda|\psi_{\lambda}\rangle$$

再次忽略与连续频谱有关的复杂度，假设其有下界，并假设能量最小下界为$E_0$，还假设我们知道最小能量对应的波函数$|\psi\rangle$

$H$的期望值为：
$$\langle\psi|H|\psi\rangle=\sum_{\lambda_1,\lambda_2\in Spec(H)}\langle\psi|\psi_{\lambda_1}\rangle \langle\psi|H|\psi_{\lambda_1}\rangle\langle\psi_{\lambda_2}|\psi\rangle$$
$$=\sum_{\lambda\in Spec(H)}\lambda|\langle\psi_{\lambda}|\psi\rangle|^2\ge\sum_{\lambda\in Spec(H)}E_{0}|\langle\psi_{\lambda}|\psi\rangle|^2=E_0$$

显然，如果我们需要遍历所有可能状态，以此最小化$H$的期望值，则最低值将是$E_0$。相应的状态则是$E_0$的本征态。这样就找到了我们要求的基态能量。但是，遍历整个希尔伯特空间通常对于物理计算来说太复杂了，计算复杂度随着体系大小N的增长指数级增大，这让精确解不太可取。

所以我们选择整个希尔伯特空间的子空间，这些子空间被一些实数可微参数$\alpha_i$（$i = 1,2,3...,N$）参数化。子空间的选择叫做拟设。某些拟设的选择可以得到比其他拟设更精确的结果，所以拟设的选择十分重要。

我们希望使用归一化的拟设，所以有约束如下：
$$\langle\psi(\alpha)|\psi(\alpha)\rangle=1$$

我们希望尽量最小化以下式子：
$$\epsilon(\alpha)=\langle\psi(\alpha)|H|\psi(\alpha)\rangle$$

一般而言这是一件困难的事，因为我们要寻找的是一个全局最小值，此时数值计算方法的复杂度会随着系统尺度极快增长。

不过以变分法为基础，我们就可以在合理时间范围内近似求解较大体系的基态波函数。

#### 马尔可夫链

马尔可夫链或马尔可夫过程是一种随机模型，描述了一系列可能的事件，其中每个事件的概率仅取决于在前一个事件中获得的状态。它以俄罗斯数学家安德烈·马尔科夫（Andrey Markov）的名字命名。马尔可夫过程是马尔可夫链蒙特卡罗的一般随机模拟方法的基础，用于模拟复杂概率分布的采样，并在贝叶斯统计、热力学、统计力学、物理、化学、经济学、金融学、信号处理、信息论和语音处理中得到应用。

马尔可夫过程是满足马尔可夫性质（有时被描述为“无记忆”）的随机过程。简单来说，这是一个可以仅根据其当前状态对未来结果进行预测的过程，最重要的是，这种预测与知道该过程完整历史的预测一样好。换句话说，以系统的当前状态为条件，其未来和过去的状态是独立的。

马尔可夫链方法对于生成随机数序列以准确反映非常复杂的理想概率分布非常重要，这个步骤通过称为马尔可夫链蒙特卡洛（MCMC）的过程来实现。

#### 马尔可夫链蒙特卡洛(MCMC)

选定合适的拟设后，我们需要构造一个马尔科夫链。我们将马尔科夫过程引入到蒙特卡洛采样，使得对波函数的抽样分布随模拟的进行而改变，即进行动态模拟。我们把马尔科夫链的平稳分布看做待估参数的后验分布，通过这条马尔科夫链产生后验分布的样本，并基于马尔科夫链达到平稳分布时的样本(有效样本)进行蒙特卡洛积分。

蒙特卡罗法(Monte Carlo method)，也称为统计模拟方法(statistical simulationmethod)，是通过从概率模型的随机抽样进行近似数值计算的方法。马尔可夫链蒙特卡罗法(Markov Chain Monte Carlo，MCMC)，则是以马尔可夫链(Markovchain)为概率模型的蒙特卡罗法。马尔可夫链蒙特卡罗法构建一个马尔可夫链，使其平稳分布就是要进行抽样的分布，首先基于该马尔可夫链进行随机游走，产生样本的序列，之后使用该平稳分布的样本进行近似的数值计算。

#### 梅特罗波利斯算法

在所有MCMC算法中，梅特罗波利斯－黑斯廷斯算法（Metropolis–Hastings algorithm），是最基础、应用最广泛的算法。

梅特罗波利斯算法可以从任何概率分布中抽取具有概率密度的样本$P(x)$，前提是我们知道函数$f(x)$和密度$P$成正比且函数值可以计算。算法只要求正比条件，这使得该算法十分有用，因为在实践中，计算必要的归一化因子通常非常困难。

梅特罗波利斯算法以Nicholas Metropolis的名字命名，他与其他三人在1953年发表的文章Exequation of State Computings by Fast Computing Machines中提出了symmetrical proposal distributions（对称分布）下的梅特罗波利斯算法。1970年时 W.K.Hastings 把该算法扩展到了更一般的情况下。

梅特罗波利斯－黑斯廷斯算法的工作原理是生成一系列样本值，随着样本值越来越多，该组值的分布更接近于所需的真实分布，也就是基态时晶格中自旋的分布。样本值通过迭代产生，下一个样本的分布仅依赖于当前样本值，也就是说样本序列是一个马尔科夫链。

具体而言，在每次迭代中，算法都会根据当前样本值为下一个样本值选取一个候选项。然后，在一定概率下，候选项要么被接受（此时候选值在下一个迭代中使用），要么被拒绝（此时候选值被丢弃，当前值在下一个迭代中被重用）接受的概率是通过比较所需分布的函数值和当前候选样本函数值来确定的。

假设P(x)为目标概率分布，则算法过程为：

1. 初始化。选定初始状态$x_0$，令t = 0
2. 迭代过程。
   1. 生成：从某一容易抽样的分布$Q(x^{'}|x_t)$中随机生成候选状态$x^{'}$；
   2. 计算：计算是否采纳候选状态的概率:
   $${\textstyle A(x'|x)=\min \left(1,{\frac {P(x')}{P(x)}}{\frac {Q(x|x')}{Q(x'|x)}}\right)}$$
   3. 接受或拒绝
       1. 从${\displaystyle [0,1]}$的均匀分布中生成随机数$u$；
       2. 如${\displaystyle u\leq A(x'|x)}{\displaystyle u\leq A(x'|x)}$，则接受该状态，并令${\displaystyle x_{t+1}=x'}{\displaystyle x_{t+1}=x'}$；
       3. 否则不接受转移，并令${\displaystyle x_{t+1}=x'}{\displaystyle x_{t}}$（复制原状态）；
   4. 增量：令$t=t+1$

此处需要提一下遍历定理：不可约、非周期且正常返的马尔可夫链，有唯一平稳分布存在，并且转移概率的极限分布是马尔可夫链的平稳分布。

因此只要保证该概率分布存在唯一平稳分布，那么在状态转移多次后，转移概率的分布就近似于我们需要的分布了。

### 伊辛模型

伊辛模型（ising model）是统计力学中铁磁性的数学模型。该模型由离散变量组成，这些变量表示原子自旋的磁偶极矩，可以处于两种状态（+1或−1）之一。自旋排列在一个晶格中，其局部结构在所有方向上周期性重复，并且允许每个自旋与其邻居相互作用。

考虑一个晶格位点集合$\wedge$，每个晶格位点都有一组相邻的位点（例如图形学中的一个连通图），总体上形成一个$d$维晶格。对于每个格点$k\in \wedge$，都有一个离散变量$\sigma_k \in \left\{+1,-1\right\}$，表示该位点的自旋。自选配置$\sigma=\left(\sigma_k\right)_{k \in \wedge}$将自旋值分配给每个晶格位点。

对于任何两个相邻的位点$i,j \in \wedge$，存在一个交互作用$J_{ij}$，此外格点$j \in \wedge$具有与之存在相互作用的外部磁场$h_j$，系统总的能量由哈密顿量给出：

$$H ( \sigma ) = -\sum _ { \langle i \space j \rangle } J _ { i j } \sigma _ { i } \sigma _ { j } - \mu \sum _ { j } h _ { j } \sigma _ { j }$$

其中第一个部分对在相邻的自旋对求和，每对计算一次。$\langle i \space j\rangle$，代表格点$i$和%$j$是最近邻的格点。磁矩由$\mu$给出。

方向一致的相邻自旋的相互作用能低于反向的自旋；系统倾向于最低的能量，但系统内的热能会干扰这种趋势，从而产生不同的相。伊辛模型允许我们把相变看做现实情况下的一个简化模型。二维方格伊辛模型是显示相变的最简单统计模型之一。

### 横场伊辛模型

横场伊辛模型（transverse field Ising model）是经典Ising模型的量子版本。它描绘了一个具有最近邻相互作用的晶格构成。沿着z轴的对称或反对称的自旋投影，以及垂直于z轴的外部磁场共同决定了这个晶格构成。不失一般性，垂直于z轴的外部磁场方向可选择x轴方向。

这个晶格配置的一个重要意义是，在量子力学角度，沿着x轴和z轴的自旋投影不是交换可观测量。也就是说它们不能被同时观测到。这意味着经典统计力学无法描述这个模型，需要量子处理。

横场伊辛模型具有以下量子哈密顿量：

$$H = - J \sum _ {\langle i , j \rangle} Z _ { i } Z _ { j } - h \sum _ { j } X _ { j } $$ 

此处下标指代晶格位点，$\sum_{\langle i,j \rangle}$对成对的格点i和j求和。$X_i$和$X_j$是自选代数元素的具体表示（自旋为$\frac 1 2$时这些元素为泡利矩阵）。在同一格点上它们交换，不同格点上则是反交换。$J$是能量维度的因子，$g = \frac Jh$是另一个耦合系数，它确定与最近邻相互作用相比外部场的相对强度。

### 后续内容

本文以2维三角晶格伊辛模型为例，介绍神经网络拟设下的量子变分解法，同时揭示算法结果与2维三角晶格伊辛模型的量子相变之间的关联。

## 神经网络拟设下的量子变分法

我们选择三角晶格伊辛模型实现我们的神经网络拟设量子变分法。由于选择了周期性边界条件，三角格子中的每个格点都与其周围6个格点都有最近邻相互作用。

### 定义哈密顿量

变分法的第一步是定义我们感兴趣的哈密顿量。为此，我们首先需要定义我们正在处理的自由度类型（比如自旋，玻色子，费米子等），这通过指定问题的希尔伯特空间来完成。

本文中，我们的研究专注于自旋$\frac12$，大小为7x7的三角晶格(图)。哈密顿量由前文可得。

### 选择拟设

定义好哈密顿量后，我们首先要找到这个哈密顿量基态的变分近似。

变分法的拟设在历史上有很多种不同选择，比如早期的均值场拟设（Mean-Field Ansatz）：

$$\langle \sigma _ { 1 } ^ { z } , \ldots \sigma _ { N } ^ { z } | \Psi _ { \mathrm { m f } } \rangle = \Pi _ { i = 1 } ^ { N } \Phi ( \sigma _ { i } ^ { z } ) ,$$ 

其中变分参数是单自旋波函数，我们可以进一步将其归一化：

$$| \Phi ( \uparrow ) | ^ { 2 } + | \Phi ( \downarrow ) | ^ { 2 } = 1 $$ 

此处可以进一步得到：

$$\Phi ( \sigma ^ { z } ) = \sqrt { P ( \sigma ^ { 2 } ) } e ^ { i \phi ( \sigma ^ { 2 } ) }$$ 

为了简化表达，我们在平均值拟设以及接下来的拟设中都取相位$\phi$为0，因为已知基态能量是正实数，所以这个处理不失一般性。

对于归一化单自旋概率，我们采用sigmoid形式函数作为近似：

$$P(\sigma_z;\lambda)= \frac 1 {1+e^{-\lambda\sigma_z}}$$

这个概率取决于实数变分参数$\lambda$。

### 变分蒙特卡洛

每个拟设可以得到一个用于初始化参数和计算结果的模型。

选择拟设后，我们现在将尝试对参数$\lambda$进行优化，以便最好地近似哈密顿量的基态。

要创建包含参数在内的变分状态，最简单的方法是构造蒙特卡洛采样的变分状态。本研究中使用上文提到的梅特罗波利斯算法。为此我们需要选择一个合适的采样器，也就是选择合适的拟设。

选定拟设后，在变分蒙特卡洛的步骤中，每一次迭代需要计算能量和梯度，然后将梯度乘以一定的学习速率，最后用这个重新缩放的梯度更新参数。

#### 随机梯度下降

在变分蒙特卡洛算法中，大部分时候都选择随机梯度下降（Stochastic gradient descent）作为迭代方法。

统计估计和机器学习都考虑了最小化具有总和形式的目标函数的问题，对于以下式子：
$$Q(\omega)=\frac 1n \sum^n_{i=1} Q_i(\omega)$$
我们想估计使得左式最小化的参数$\omega$。
其中每个求和函数$Q_i(\omega)$与数据集中第i个观测值相关联。

然而，在统计学中，人们早就认识到，对于一些最大似然估计的问题，即使只是要求局部最小化限制性也太强。因此当代统计学家经常考虑似然函数的稳态点（或使用打分函数等近似方程）。

当用于最小化上式时，标准梯度下降方法将执行以下迭代：

$${\displaystyle w：=w-\eta \nabla Q(w)=w-{\frac {\eta }{n}}\sum _{i=1}^{n}\nabla Q_{i}(w)}$$

其中$\eta$是步长大小，在机器学习里称作学习率。

在许多情况下，被加函数具有简单的形式，可以对 sum 函数和与梯度进行廉价的求和。例如，在统计学中，单参数指数族允许经济的函数评估和梯度评估。

但是，在其他情况下，计算总和梯度可能需要对所有求和函数的梯度进行昂贵的评估。当训练集很大且不存在简单公式时，计算梯度之和变得非常昂贵，因为计算梯度需要计算所有求和函数的梯度。为了节省每次迭代的计算成本，随机梯度下降在每一步都会对求和函数的子集进行采样。这在大规模机器学习问题的情况下非常有效。这种方法就是随机梯度下降

随机梯度下降可以视为梯度下降优化的随机近似，因为它将实际梯度（意味着根据整个数据集计算）替换为其估计值（意味着根据随机选择的数据集子集计算）。特别是在高维优化问题中，这减少了非常高的计算负担，实现了更快的迭代，从而降低了收敛率。

在随机梯度下降中，真实梯度${Q(w)}$近似于单个样本的梯度：

$${\displaystyle w：=w-\eta \nabla Q(w)=w-\eta  \nabla Q_{i}(w)}$$

当算法遍历训练集时，它会对每个训练样本执行上述更新。在算法收敛之前，可以在训练集上进行多次传递。为了防止循环，可以对每次传递的数据进行随机排列。

随机梯度下降的伪代码可表示如下：

1. 选择参数的初始向量$w$和学习率$\eta$。
2. 重复以下述步骤，直到获得近似的最小值：
   1. 随机随机排列训练集中的样本。
   2. 为$i=1，2,...,n$做一下更新：
$w：=w-\eta \nabla Q_{i}(w)$

随机梯度下降的收敛性已使用凸最小化和随机近似理论进行了分析。简而言之，当学习速率 $\eta$以适当的速率减小，并且受制于相对温和的假设：当目标函数为凸函数或伪凸函数时，随机梯度下降几乎肯定会收敛到全局最小值，否则几乎肯定会收敛到局部最小值。

### 神经网络量子态（NNQS）

基于波函数的神经网络表示，我们可以使用一个更精确的拟设。在前文自动微分的基础上，我们只需要编写一个神经网络量子态采样器，就可以实现我们的目的。

考虑一个具有N个离散值自由度的量子系统，多体波函数是一个从N维集合到指数级别复杂度的复数的映射。该映射可完全表明量子态的相位和振幅。

神经网络量子态（NQS或NNQS）是一类通用的变分量子态，根据人工神经网络进行参数化。它于2017年由物理学家Giuseppe Carleo和Matthias Troyer[1]首次引入，用于近似多体量子系统的波函数。

给定一个多体量子体系的量子态$|\Psi\rangle$和体系的自由度$N$以及一组相关量子数$s_1...s_N$，则神经网络量子态可以如下形式参数化波函数振幅：

$$\langle s_1...s_N|\Psi;W\rangle=F(s_1...s_N;W)$$

其中$F(s_1...s_N;W)$是一个神经网络，以权重$W$，$N$个输入变量$(s_1...s_N)$为参数。这种变分形式可以与特定的随机学习方法结合使用，以近似目标量子态。

#### 全连接前反馈神经网络（fully-connected feed-forward network）

前馈神经网络是第一种也是最简单的人工神经网络。在这个网络中，信息只在一个方向上向前移动—：从输入节点，通过隐藏节点到输出节点。网络中没有循环或环路。

我们使用其中的特例：多层感知机（MLP），在前反馈神经网络的基础上，多层感知机一层中的每个神经元都具有与后续层的神经元的定向连接。由于MLP是完全连接的，因此一层中的每个节点都以一定的权重连接$w_{ij}$到下一层中的每个节点。

如果多层感知器在所有神经元中都有线性激活函数，即将加权输入映射到每个神经元输出的线性函数，则线性代数表明任何数量的层都可以简化为两层输入输出模型。在MLP中，一些神经元使用非线性激活函数，该函数被开发用于模拟生物神经元的动作电位或放电的频率。

历史上常见的激活函数是sigmoids，并且描述如下：

$${f（x）={\frac {1}{1+e^{-x}}}}$$

机器学习中，当使用基于梯度的学习方法和反向传播训练人工神经网络时，可能会遇到梯度消失问题。在这样的方法中，在训练的每次迭代期间，神经网络的每个权重都会收到与误差函数相对于当前权重的偏导数成比例的更新。问题在于，在某些情况下，梯度会变得非常小，从而有效地阻止了权重改变其值。在最坏的情况下，这可能会完全阻止神经网络进一步训练。

sigmoid激活函数在小范围之外具有非常小的导数值，并且由于梯度消失问题，在一些深度神经网络中不能很好地工作。

在深度学习的最新发展中，整流器线性单元(ReLU or Rectified Linear Unit)更常被用作克服与sigmoids相关的数值问题的可能方法之一:

$${f(x)=x^{+}=\max(0,x)}$$

具体到本研究中，我们定义一个多层感知机，其中使用一个求和层，并使用ReLu函数作为激活函数。

#### 学习基态波函数

NNQS的一个常见应用是找到给定哈密顿量$\hat(H)$的基态波函数的近似表示。这与我们的目的不谋而合。此时学习过程为找到最佳的神经网络权重，以最小化变分能量：

$$ E（W）=\langle \Psi ;W|{\hat {H}}|\Psi ;W\rangle $$

对于一般的人工神经网络，计算期望值是一个代价高昂的指数级操作。具体而言，一个含有M个样本$S^{(1)},S^{(2)}...S^{(M)}$的集合,其中：

$$S^{(i)}=s_1^{(i)}...s_N^{(i)}$$

该组样本被生成，并使其满足Born分布：

$${ P（S）\propto |F（s_{1}\ldots s_{N};W）|^{2}}$$

可以证明，所谓局部能量的样本均值${E_{\mathrm {loc} }（S）=\langle S|{\hat {H}}|\Psi \rangle /\langle S|\Psi \rangle }$是量子期望值$E(W)$的统计估计值：

$${E（W）\simeq {\frac {1}{M}}\sum _{i}^{M}E_{\mathrm {loc} }（S^{（i）}）}$$

类似地，可以证明以网络权重W为参数的能量梯度也可以由样本均值近似：

$${{\frac {\partial E（W）}{\partial W_{k}}}\simeq {\frac {1}{M}}\sum _{i}^{M}（E_{\mathrm {loc} }（S^{（i）}）-E（W））O_{k}^{\star }（S^{（i）}），}$$

其中${O（S^{（i）}）={\frac {\partial \log F（S^{（i）};W）}{\partial W_{k}}}}$且可以通过反向传播算法计算。

之后使用随机梯度下降算法来最小化能量$E(W)$。当神经网络参数在学习过程的每一步更新时，一组新的样本${S^{（i）}}$在类似于无监督学习中所做的迭代过程中逐一生成。形成一个马尔科夫链。迭代次数达到一定程度后，整个马尔科夫链收敛到稳态。此时就计算出了基态时的波函数能量。

由结果对比可知，均值场近似误差在2%左右，而神经网络量子态的误差则小一些。

在较小的系统中，精确求解法比如精确对角化可用。

比较4X4三角晶格中精确对角化结果与神经网络量子态的结果，可发现误差较小。这为我们之后的其他计算的准确性带来了依据。

## 三角晶格伊辛模型量子相变

### 各类物理量

神经网络量子态（NQS）拟设下的变分蒙特卡洛方法计算出的能量，在较小晶格下与数值解基本吻合，因此我们推测在较大三角晶格量子体系中，该算法依旧适用。

NQS不但可以计算能量，还能求出其他物理量的近似解，这些解与量子多体体系的一些性质密切相关。

### 

### 结构因子与相变点

## 结论