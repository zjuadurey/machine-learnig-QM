{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netket as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetKet version: 3.3.2.post2\n"
     ]
    }
   ],
   "source": [
    "print(f\"NetKet version: {nk.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "hi = nk.hilbert.Spin(s=1 / 2, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
       "              -1., -1., -1., -1.,  1.,  1., -1., -1.],\n",
       "             [ 1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,\n",
       "               1.,  1.,  1.,  1., -1.,  1., -1.,  1.],\n",
       "             [-1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
       "               1.,  1.,  1.,  1., -1.,  1., -1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "hi.random_state(jax.random.PRNGKey(0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netket.operator.spin import sigmax,sigmaz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = -1\n",
    "H = sum([Gamma*sigmax(hi,i) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=-1\n",
    "H += sum([V*sigmaz(hi,i)*sigmaz(hi,(i+1)%N) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp_h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/machine-learning-QM/1.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/usr/local/machine-learning-QM/1.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m eigsh\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/usr/local/machine-learning-QM/1.ipynb#ch0000024vscode-remote?line=2'>3</a>\u001b[0m eig_vals, eig_vecs \u001b[39m=\u001b[39m eigsh(sp_h, k\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, which\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSA\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/usr/local/machine-learning-QM/1.ipynb#ch0000024vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39meigenvalues with scipy sparse:\u001b[39m\u001b[39m\"\u001b[39m, eig_vals)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/usr/local/machine-learning-QM/1.ipynb#ch0000024vscode-remote?line=6'>7</a>\u001b[0m E_gs \u001b[39m=\u001b[39m eig_vals[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sp_h' is not defined"
     ]
    }
   ],
   "source": [
    "sp_h=H.to_sparse()\n",
    "sp_h.shape\n",
    "\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "eig_vals, eig_vecs = eigsh(sp_h, k=2, which=\"SA\")\n",
    "\n",
    "print(\"eigenvalues with scipy sparse:\", eig_vals)\n",
    "\n",
    "E_gs = eig_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model. \n",
    "# Notice that this does not create the parameters.\n",
    "mf_model=MF()\n",
    "\n",
    "# Create the local sampler on the hilbert space\n",
    "sampler = nk.sampler.MetropolisLocal(hi)\n",
    "\n",
    "# Construct the variational state using the model and the sampler above.\n",
    "# n_samples specifies how many samples should be used to compute expectation\n",
    "# values.\n",
    "vstate = nk.vqs.MCState(sampler, mf_model, n_samples=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    lambda: DeviceArray([-0.00082106], dtype=float64),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# you can inspect the parameters which contain the single\n",
    "# variational parameter `lambda`\n",
    "print(vstate.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.86 ± 0.20 [σ²=20.11, R̂=0.9937]\n"
     ]
    }
   ],
   "source": [
    "# Expectation value: notice that it also provides an error estimate.\n",
    "E = vstate.expect(H)\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean                  : -19.859484128455726\n",
      "Error                 : 0.19816453949554358\n",
      "Variance              : 20.105822573302195\n",
      "Convergence indicator : 0.9936760107778482\n",
      "Correlation time      : 0.0\n"
     ]
    }
   ],
   "source": [
    "# the energy (expectation value) is a structure with a lot of fields:\n",
    "print(\"Mean                  :\", E.mean)\n",
    "print(\"Error                 :\", E.error_of_mean)\n",
    "print(\"Variance              :\", E.variance)\n",
    "print(\"Convergence indicator :\", E.R_hat)\n",
    "print(\"Correlation time      :\", E.tau_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-19.86 ± 0.20 [σ²=20.11, R̂=0.9937],\n",
       " FrozenDict({\n",
       "     lambda: DeviceArray([-0.18306634], dtype=float64),\n",
       " }))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vstate.expect_and_grad(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Variational Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \n",
    "    # You can define attributes at the module-level\n",
    "    # with a default. This allows you to easily change\n",
    "    # some hyper-parameter without redefining the whole \n",
    "    # flax module.\n",
    "    alpha : int = 1\n",
    "            \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # here we construct the first dense layer using a\n",
    "        # pre-built implementation in flax.\n",
    "        # features is the number of output nodes\n",
    "        # WARNING: Won't work with complex hamiltonians because\n",
    "        # of a bug in flax. Use nk.nn.Dense otherwise. \n",
    "        dense = nn.Dense(features=self.alpha * x.shape[-1])\n",
    "        \n",
    "        # we apply the dense layer to the input\n",
    "        y = dense(x)\n",
    "\n",
    "        # the non-linearity is a simple ReLu\n",
    "        y = nn.relu(y)\n",
    "                \n",
    "        # sum the output\n",
    "        return jnp.sum(y, axis=-1)\n",
    "    \n",
    "model = FFN(alpha=1)\n",
    "\n",
    "vstate = nk.vqs.MCState(sampler, model, n_samples=1008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:05<00:00, 52.92it/s, Energy=-25.459 ± 0.011 [σ²=0.118, R̂=1.0007]]   \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eig_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/machine-learning-QM/1.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/usr/local/machine-learning-QM/1.ipynb#ch0000016vscode-remote?line=6'>7</a>\u001b[0m gs\u001b[39m.\u001b[39mrun(n_iter\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m,out\u001b[39m=\u001b[39mlog)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/usr/local/machine-learning-QM/1.ipynb#ch0000016vscode-remote?line=8'>9</a>\u001b[0m ffn_energy\u001b[39m=\u001b[39mvstate\u001b[39m.\u001b[39mexpect(H)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/usr/local/machine-learning-QM/1.ipynb#ch0000016vscode-remote?line=9'>10</a>\u001b[0m error\u001b[39m=\u001b[39m\u001b[39mabs\u001b[39m((ffn_energy\u001b[39m.\u001b[39mmean\u001b[39m-\u001b[39meig_vals[\u001b[39m0\u001b[39m])\u001b[39m/\u001b[39meig_vals[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/usr/local/machine-learning-QM/1.ipynb#ch0000016vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOptimized energy and relative error: \u001b[39m\u001b[39m\"\u001b[39m,ffn_energy,error)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eig_vals' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = nk.optimizer.Sgd(learning_rate=0.1)\n",
    "\n",
    "# Notice the use, again of Stochastic Reconfiguration, which considerably improves the optimisation\n",
    "gs = nk.driver.VMC(H, optimizer, variational_state=vstate,preconditioner=nk.optimizer.SR(diag_shift=0.1))\n",
    "\n",
    "log=nk.logging.RuntimeLog()\n",
    "gs.run(n_iter=300,out=log)\n",
    "\n",
    "ffn_energy=vstate.expect(H)\n",
    "error=abs((ffn_energy.mean-eig_vals[0])/eig_vals[0])\n",
    "print(\"Optimized energy and relative error: \",ffn_energy,error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jastrow = log.data\n",
    "print(data_jastrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.errorbar(data_jastrow[\"Energy\"].iters, data_jastrow[\"Energy\"].Mean, yerr=data_jastrow[\"Energy\"].Sigma)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. N-N Quantum State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \n",
    "    # You can define attributes at the module-level\n",
    "    # with a default. This allows you to easily change\n",
    "    # some hyper-parameter without redefining the whole \n",
    "    # flax module.\n",
    "    alpha : int = 1\n",
    "            \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # here we construct the first dense layer using a\n",
    "        # pre-built implementation in flax.\n",
    "        # features is the number of output nodes\n",
    "        # WARNING: Won't work with complex hamiltonians because\n",
    "        # of a bug in flax. Use nk.nn.Dense otherwise. \n",
    "        dense = nn.Dense(features=self.alpha * x.shape[-1])\n",
    "        \n",
    "        # we apply the dense layer to the input\n",
    "        y = dense(x)\n",
    "\n",
    "        # the non-linearity is a simple ReLu\n",
    "        y = nn.relu(y)\n",
    "                \n",
    "        # sum the output\n",
    "        return jnp.sum(y, axis=-1)\n",
    "    \n",
    "model = FFN(alpha=1)\n",
    "\n",
    "vstate = nk.vqs.MCState(sampler, model, n_samples=1008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nk.optimizer.Sgd(learning_rate=0.1)\n",
    "\n",
    "# Notice the use, again of Stochastic Reconfiguration, which considerably improves the optimisation\n",
    "gs = nk.driver.VMC(H, optimizer, variational_state=vstate,preconditioner=nk.optimizer.SR(diag_shift=0.1))\n",
    "\n",
    "log=nk.logging.RuntimeLog()\n",
    "gs.run(n_iter=300,out=log)\n",
    "\n",
    "ffn_energy=vstate.expect(H)\n",
    "error=abs((ffn_energy.mean-eig_vals[0])/eig_vals[0])\n",
    "print(\"Optimized energy and relative error: \",ffn_energy,error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_FFN = log.data\n",
    "\n",
    "plt.errorbar(data_jastrow[\"Energy\"].iters, data_jastrow[\"Energy\"].Mean, yerr=data_jastrow[\"Energy\"].Sigma, label=\"Jastrow\")\n",
    "plt.errorbar(data_FFN[\"Energy\"].iters, data_FFN[\"Energy\"].Mean, yerr=data_FFN[\"Energy\"].Sigma, label=\"FFN\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
